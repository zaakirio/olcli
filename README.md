# ü¶ô Ollama Multi-Chat CLI

A powerful command-line interface for simultaneous conversations with multiple Ollama AI models. Compare responses, leverage different model strengths, and manage your AI interactions with profiles, templates, and chat persistence.

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Node](https://img.shields.io/badge/node-%3E%3D22.0.0-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

## ‚ú® Features

### üöÄ **Multi-Model Chat**
- Chat with multiple Ollama models simultaneously
- Side-by-side response comparison
- Real-time streaming responses
- Optimized performance with reduced screen flickering

### üë§ **Profiles System**
- Pre-built personas (Senior Architect, Developer, Code Reviewer, Teacher)
- Custom profile creation with system prompts
- Automatic context injection into conversations
- Profile management (create, view, delete)

### üìã **Templates**
- Pre-configured model collections (General, Coding, Creative, Analysis)
- Custom template creation
- Intelligent model availability detection
- Quick-start with curated model sets

### ‚¨áÔ∏è **Model Management**
- Install new Ollama models directly from CLI
- Progress tracking for downloads
- Popular model suggestions
- Error handling and retry logic

### üíæ **Chat Persistence**
- Export conversations with full metadata
- Import previous chat sessions
- Preserve profiles and templates in exports
- Organized file storage system

## üîß Prerequisites

- **Node.js 22+** (required for ESM support)
- **Ollama** installed and running locally
- At least one Ollama model downloaded

### Installing Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

### Installing Models

```bash
# Popular lightweight models
ollama pull llama3.2:3b
ollama pull phi3:mini
ollama pull qwen2.5:7b

# Coding-focused models
ollama pull deepseek-coder:6.7b
ollama pull codellama:7b
```

## üì¶ Installation

### Option 1: Install from npm (Recommended)
```bash
# Install globally
npm install -g ollama-multi-chat

# Run the CLI
olcli
```

### Option 2: Clone and Build
```bash
git clone <repository-url>
cd ollama-cli
npm install
npm run build
npm link  # Optional: for global installation (enables 'olcli' command)
```

### Option 3: Direct Usage
```bash
npm install
npm run dev  # Development mode with hot reload
```

## üöÄ Quick Start

```bash
# Option 1: Install from npm (Recommended)
npm install -g ollama-multi-chat
olcli

# Option 2: Local development
git clone <repository-url>
cd ollama-cli
npm install
npm run build
npm link  # Enables 'olcli' command globally
olcli
```

## üöÄ Usage

### Starting the Application
```bash
# If globally installed (recommended)
olcli

# Or run directly from project directory
npm start

# Development mode with hot reload
npm run dev

# Get help
olcli --help

# Show version
olcli --version

# List available models
olcli --list-models
```

### Main Menu Navigation

The application starts with a main menu offering these options:

```
ü¶ô Ollama Multi-Chat CLI
Chat with multiple AI models simultaneously

Current Settings:
Profile: Senior Solutions Architect
Template: Code Assistant

Main Menu:
‚Üí üöÄ Start Chat Session
  üë§ Manage Profiles  
  üìã Manage Templates
  ‚¨áÔ∏è Install Models
  üíæ Export/Import Chats
```

## üìö Feature Guide

### üöÄ Starting a Chat Session

1. Select **"Start Chat Session"** from main menu
2. Choose models using arrow keys and spacebar
3. Press Enter to start chatting
4. Type messages and see responses from all selected models
5. Use ESC to return to model selection

**Chat Controls:**
- `ESC` - Back to model selection
- `Enter` - Send message
- Real-time streaming responses

### üë§ Managing Profiles

Profiles add context and personality to your AI interactions:

**Built-in Profiles:**
- **Senior Solutions Architect** - System design expertise
- **Full-Stack Developer** - Practical coding solutions  
- **Code Reviewer** - Focus on quality and best practices
- **Technical Teacher** - Clear explanations and examples

**Creating Custom Profiles:**
1. Select **"Manage Profiles"**
2. Choose **"+ Create New Profile"**
3. Enter name, description, and system prompt
4. Profile automatically applies to new conversations

### üìã Using Templates

Templates provide curated model collections for specific use cases:

**Built-in Templates:**
- **General Purpose** - `llama3.2:3b`, `phi3:mini`, `qwen2.5:7b`
- **Code Assistant** - `deepseek-coder:6.7b`, `codellama:7b`, `starcoder2:3b`
- **Creative Writing** - `llama3.2:3b`, `mistral:7b`, `gemma2:9b`
- **Data Analysis** - `qwen2.5:7b`, `llama3.1:8b`, `phi3:medium`

**Creating Custom Templates:**
1. Select **"Manage Templates"**
2. Choose **"+ Create New Template"**
3. Enter name and description
4. Select models for the template
5. Template appears in future selections

### ‚¨áÔ∏è Installing Models

Add new models without leaving the application:

```
Model Name: llama3.2:3b
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 65%
Downloading...

Popular Models:
‚Ä¢ llama3.2:3b - Fast, lightweight general-purpose model
‚Ä¢ phi3:mini - Microsoft's compact model  
‚Ä¢ qwen2.5:7b - Alibaba's advanced model
‚Ä¢ deepseek-coder:6.7b - Specialized for coding
```

### üíæ Chat Export/Import

Preserve and share your conversations:

**Exporting:**
1. During or after a chat session
2. Select **"Export/Import Chats"** ‚Üí **"Export Current Chat"**
3. Enter a descriptive name
4. File saved to `~/.ollama-multi-chat/exports/`

**Importing:**
1. Select **"Import Chat"** from export menu
2. Choose from available exported chats
3. Conversation, profile, and template restored
4. Continue where you left off

## ‚öôÔ∏è Configuration

### Storage Locations

All data stored in `~/.ollama-multi-chat/`:
```
~/.ollama-multi-chat/
‚îú‚îÄ‚îÄ profiles.json          # User profiles
‚îú‚îÄ‚îÄ templates.json         # Model templates  
‚îî‚îÄ‚îÄ exports/               # Exported chats
    ‚îú‚îÄ‚îÄ chat-session-1.json
    ‚îî‚îÄ‚îÄ chat-session-2.json
```

### Model Timeouts

Automatic timeouts based on model size:
- **Small models** (3B params): 2 minutes
- **Medium models** (7-13B): 3 minutes  
- **Large models** (24B+): 5 minutes

## üîß Development

### Scripts
```bash
npm run build      # Compile TypeScript
npm run dev        # Development with hot reload
npm run typecheck  # Type checking only
npm run lint       # ESLint checking
npm test          # Run tests
```

### Project Structure
```
src/
‚îú‚îÄ‚îÄ components/           # React components
‚îÇ   ‚îú‚îÄ‚îÄ ChatInterface.tsx
‚îÇ   ‚îú‚îÄ‚îÄ ModelSelector.tsx
‚îÇ   ‚îú‚îÄ‚îÄ ProfileManager.tsx
‚îÇ   ‚îú‚îÄ‚îÄ TemplateManager.tsx
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ services/            # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ ollama.ts       # Ollama API client
‚îÇ   ‚îî‚îÄ‚îÄ storage.ts      # Data persistence
‚îú‚îÄ‚îÄ types.ts            # TypeScript definitions
‚îî‚îÄ‚îÄ App.tsx            # Main application
```

## üêõ Troubleshooting

### Common Issues

**"Failed to connect to Ollama"**
```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve
```

**"No models available"**
```bash
# Install a model first
ollama pull llama3.2:3b

# Verify installation
ollama list
```

**"Model timeout errors"**
- Large models need time to load initially
- Subsequent requests are faster
- Consider using smaller models for testing

**"Screen flickering during responses"**
- Updated with performance optimizations
- Reduced update frequency and debouncing
- Contact support if issues persist

### Performance Tips

1. **Start with smaller models** (3B parameters) for faster responses
2. **Use templates** to quickly switch between model sets
3. **Profile system** reduces repetitive context setting
4. **Export important chats** to preserve valuable conversations

## üì¶ Publishing to npm

This project uses semantic versioning and automated publishing:

### Version Management
```bash
# Patch version (1.0.0 -> 1.0.1)
npm version patch

# Minor version (1.0.0 -> 1.1.0)
npm version minor

# Major version (1.0.0 -> 2.0.0)
npm version major
```

### Publishing Process
```bash
# 1. Ensure you're logged in to npm
npm login

# 2. Update version (automatically builds and commits)
npm version patch

# 3. Publish to npm
npm publish

# Or publish with tag
npm publish --tag beta
```

### Pre-publish Checks
The following happens automatically before publishing:
- TypeScript type checking (`npm run typecheck`)
- Build process (`npm run build`)
- Git commit and tag creation

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **Ollama** team for the excellent local AI model runtime
- **Ink** for the beautiful terminal UI framework
- **React** for the component architecture
- Contributors and testers

---

**Built with ‚ù§Ô∏è for the AI development community**

For issues, feature requests, or questions, please open an issue on GitHub.